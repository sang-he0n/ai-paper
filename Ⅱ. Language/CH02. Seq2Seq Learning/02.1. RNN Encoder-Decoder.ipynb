{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1053003b",
   "metadata": {},
   "source": [
    "# **CH02.1. RNN-base Encoder-decoder**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877c55b9",
   "metadata": {},
   "source": [
    "#### **`Paper Info`** : Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation\n",
    "#### $ \\hspace{1.75cm} - $ Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre et al. (EMNLP/2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73552354",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81741fe0",
   "metadata": {},
   "source": [
    "> ## **요약(Summary)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07848aea",
   "metadata": {},
   "source": [
    "| Item | Description |\n",
    "|------|-------------|\n",
    "| Research topic | RNN Encoder–Decoder로 phrase/sentence-level conditional modeling을 수행하여 SMT의 phrase table scoring을 개선 |\n",
    "| Core idea | Encoder가 variable-length source를 fixed-length vector $ c $로 압축하고 Decoder가 $ p(y \\mid x) $를 autoregressive하게 <br> 모델링하며 새로운 gated hidden unit으로 안정적 학습과 장단기 의존성 포착 | \n",
    "| Key findings | $ \\cdot{} $ Log-linear SMT에 RNN score를 feature로 추가 시 BLEU 상승 <br> $ \\cdot{} $ 빈도 의존 통계 대신 linguistic regularity를 반영하는 representation 학습 <br> $ \\cdot{} $ Long/rare phrase에서 특히 이득 |\n",
    "| Contributions | $ \\cdot{} $ RNN Encoder–Decoder 아키텍처 제안 <br> $ \\cdot{} $ Reset/Update gates를 갖는 간결한 unit 제안(초기 GRU) <br> $ \\cdot{} $ SMT phrase pair rescoring으로 상보적 성능 향상 실증 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7067ccf",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371bb85",
   "metadata": {},
   "source": [
    "> ## **연구 배경(Motivation & Prior Work)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802949e2",
   "metadata": {},
   "source": [
    "#### **(1) 기존 연구 및 한계점** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Phrase-based SMT는 주로 빈도 기반 phrase translation probability와 language model을 결합하여 log-linear로 디코딩하지만 rare/long phrase에서 일반화가 약함\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Feedforward NN 기반 접근은 fixed-size 입력/출력 제약으로 최대 길이를 작게 두어야 하고 word order 반영이 제한적임\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Word2Vec은 distributional similarity에 강점이 있으나 conditional generation과 sequence-to-sequence mapping에 직접적이지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eaf9af9",
   "metadata": {},
   "source": [
    "#### **(2) 연구 목표** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Variable-length 시퀀스를 보존된 order로 인코딩하고 조건부 분포 $ p(y \\mid{} x) $를 직접 학습하여 SMT에서 의미·통사적 규칙을 반영하는 phrase scoring 제공\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 기존 빈도 기반 통계의 한계를 보완하여 rare/long phrase에서도 일관된 품질 확보"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612c9171",
   "metadata": {},
   "source": [
    "#### **(3) 제안된 방법론** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ RNN Encoder–Decoder를 학습하여 source phrase $ x $를 고정 길이 표현 $ c $로 요약하고 decoder로 target phrase $ y $의 조건부분포를 모델링하여 log-linear SMT에 추가 feature로 사용\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Reset/Update gate를 갖는 간결한 gated unit을 채택하여 장기 정보 보존과 선택적 갱신을 동시에 달성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b42efb",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ec7d5c",
   "metadata": {},
   "source": [
    "> ## **방법론(Method)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b20d406",
   "metadata": {},
   "source": [
    "#### **(1) RNN Language Modeling** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 입력 시퀀스 $ x = (x_{1}, \\ldots, x_{T}) $에 대해 은닉 상태는 $ h_{t} = f(h_{t-1}, x_{t}) $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 시점별 조건부분포는 $ p(x_{t} \\mid x_{<t}) = \\text{softmax}\\big(W_{\\text{out}} h_{t}\\big) $\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} p(x) = \\prod_{t=1}^{T} p(x_{t} \\mid x_{<t}) \\;\\; \\text{ where } \\, h_{t} \\in{} \\mathbb{R}^{H}, \\; W_{\\text{out}} \\in{} \\mathbb{R}^{V \\times H} $\n",
    "#### **(2) Encoder–Decoder** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Encoder는 $ h^{\\text{enc}}_{t} = f^{\\text{enc}}(h^{\\text{enc}}_{t-1}, x_{t}) $로 순회 후 요약벡터 $ c = \\psi{}(h^{\\text{enc}}_{T}) $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Decoder는 $ h^{\\text{dec}}_{t} = f^{\\text{dec}}(h^{\\text{dec}}_{t-1}, y_{t-1}, c) $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 다음 심볼 조건분포는 $ p(y_{t} \\mid y_{<t}, c) = \\text{softmax}\\big(g(h^{\\text{dec}}_{t}, y_{t-1}, c)\\big) $\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} p(y \\mid x) = \\prod_{t=1}^{T'} p(y_{t} \\mid y_{<t}, c) \\;\\; \\text{ where } \\, c \\in{} \\mathbb{R}^{C}, \\; g: \\mathbb{R}^{H} \\to \\mathbb{R}^{V} $\n",
    "#### **(3) 학습목표(Objective)** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 병렬 phrase 쌍 데이터 $ \\mathcal{D} = \\{(x^{(n)}, y^{(n)})\\}_{n=1}^{N} $\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} \\displaystyle{} \\max_{\\theta{}} \\; \\sum_{n=1}^{N} \\log{} p_{\\theta{}}\\big(y^{(n)} \\mid x^{(n)}\\big) \\;\\; \\text{ where } \\, \\theta{} : \\text{encoder/decoder parameters} $\n",
    "#### **(4) Gated hidden unit** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Reset gate $ r_{t} = \\sigma{}\\big(W_{r} x_{t} + U_{r} h_{t-1}\\big) $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Update gate $ z_{t} = \\sigma{}\\big(W_{z} x_{t} + U_{z} h_{t-1}\\big) $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 후보 상태 $ \\tilde{h}_{t} = \\phi{}\\big(W x_{t} + U(r_{t} \\odot h_{t-1})\\big) $\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} h_{t} = z_{t} \\odot h_{t-1} + (1 - z_{t}) \\odot \\tilde{h}_{t} \\;\\; \\text{ where } \\, \\sigma{} : \\text{logistic}, \\; \\phi{} : \\tanh{}, \\; \\odot : \\text{Hadamard} $\n",
    "##### **(`PLUS`) 게이트 해석** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ $ z_{t} \\to 1 $이면 항등 경로가 열려 장기 정보가 보존되고 $ z_{t} \\to 0 $이면 새로운 정보로 갱신\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ $ r_{t} \\to 0 $이면 과거 의존을 억제하여 단기 패턴에 집중\n",
    "#### **(5) SMT 통합(Log-linear model)** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ SMT 디코딩은 $ \\displaystyle{} \\arg\\max_{y} \\sum_{k} w_{k} f_{k}(x,y) $\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} \\text{여기에 } f_{\\text{RNN}}(x,y) = \\log{} p_{\\theta{}}(y \\mid x) \\text{ 를 추가하여 가중치 } w_{k} \\text{ 는 개발셋 BLEU로 튜닝 } \\;\\; \\text{ where } \\, f_{k} : \\text{기존 TM/LM/penalty 등} $\n",
    "##### **(`PLUS`) Frequency handling** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 학습 샘플링 시 phrase pair 빈도를 무시하여 RNN이 빈도 랭킹이 아닌 linguistic regularity를 학습하도록 유도\n",
    "##### **(`PLUS`) Decoder 출력층** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Maxout + softmax를 사용하여 표현력과 안정성을 확보"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed232f67",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7e6b02",
   "metadata": {},
   "source": [
    "> ## **실험(Experiments)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245a318f",
   "metadata": {},
   "source": [
    "#### **(1) 실험 설정** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 데이터셋\n",
    "| Item | Description |\n",
    "|------|-------------|\n",
    "| Task | WMT14 En→Fr |\n",
    "| Parallel words | $\\approx{} 850M$ |\n",
    "| LM raw words | $\\approx{} 2B$ |\n",
    "| Data selection | LM $418M$ words used from $2B$ |\n",
    "| RNN training words | $348M$ from parallel subset |\n",
    "| Dev set | newstest2012/2013 |\n",
    "| Test set | newstest2014 |\n",
    "| Vocab | $15k$ En, $15k$ Fr, OOV→[UNK] |\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 모델 및 훈련\n",
    "| Item | Description |\n",
    "|------|-------------|\n",
    "| Encoder/Decoder units | $1000$ gated units each |\n",
    "| Embedding | rank-$100$ (effective dim $100$) |\n",
    "| Decoder output | Maxout-$500$ (pool=2) + softmax |\n",
    "| Optimizer | Adadelta($\\rho{}=0.95,\\epsilon{}=1e{-6}$) + SGD |\n",
    "| Batch size | $64$ phrase pairs |\n",
    "| Init | $ \\mathcal{N}(0,0.01) $, recurrent via SVD left-singular vectors |\n",
    "| Train time | $\\approx{} 3$ days (GPU) |\n",
    "| CSLM baseline | 7-gram, emb $512$, ReLU layers, val ppl $45.80$ |\n",
    "| Integration | Log-linear feature add (CSLM+RNN complementary) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42fa76ae",
   "metadata": {},
   "source": [
    "#### **(2) 정량 분석 요약** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Baseline SMT 대비 RNN feature 추가로 dev/test BLEU 모두 상승\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ CSLM + RNN 조합이 최상 성능을 보여 상보적임을 확인\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Word penalty를 추가하면 dev에서 미세 이득이나 test에선 제한적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaef28f",
   "metadata": {},
   "source": [
    "#### **(3) 정성 분석 요약** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Long/rare source phrase에서 RNN score는 의미·통사 규칙을 더 잘 반영하여 자연스러운 target 후보를 상위에 배치\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 통계 모델은 빈도 강점, RNN은 regularity 강점으로 서로 보완적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af642ab",
   "metadata": {},
   "source": [
    "##### **(`PLUS`) Representation 시각화** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ T-SNE로 본 단어/문구 임베딩에서 의미·통사적 유사성에 따른 군집 형성 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1147fd",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e4be05",
   "metadata": {},
   "source": [
    "> ## **결론(Conclusion)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1b5fb1",
   "metadata": {},
   "source": [
    "#### **(1)** RNN Encoder–Decoder는 phrase-level $ p(y \\mid x) $ 모델링으로 SMT의 빈도 편향을 보완\n",
    "#### **(2)** Reset/Update gate를 갖는 간결한 unit으로 장단기 의존성 학습과 안정적 최적화를 달성\n",
    "#### **(3)** Log-linear SMT에 RNN score를 feature로 추가하면 CSLM과 상보적으로 BLEU가 상승\n",
    "#### **(4)** Long/rare phrase에서 linguistic regularity를 포착하는 representation 학습의 이점을 정량·정성으로 확인\n",
    "#### **(5)** 향후 attention 도입 및 phrase table 대체 가능성으로 확장 여지가 큼"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
