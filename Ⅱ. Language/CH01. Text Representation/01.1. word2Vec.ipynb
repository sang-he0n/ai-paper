{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1053003b",
   "metadata": {},
   "source": [
    "# **CH01.1. Word2Vec**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22532338",
   "metadata": {},
   "source": [
    "#### **`Paper Info`** : Efficient Estimation of Word Representations in Vector Space\n",
    "#### $ \\hspace{1.75cm} - $ Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean (ICLR/2013)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9239f43",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8475e4",
   "metadata": {},
   "source": [
    "> ## **요약(Summary)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35742aa9",
   "metadata": {},
   "source": [
    "| Item | Description |\n",
    "|------|-------------|\n",
    "| Research topic | 대규모 말뭉치 환경에서 효율적으로 단어 분산 표현을 학습하기 위한 log-linear 기반 CBOW·Skip-gram 모델 제안 |\n",
    "| Key findings | $ \\cdot{} $ (Word2Vec은) Non-linear hidden layer 제거 및 Hierarchical Softmax로 NNLM, RNNLM 대비 sample 별 학습 연산량 대폭 감소 <br> $ \\cdot{} $ 문맥 확률 분포 학습 효과로 학습된 임베딩이 의미, syntactic 유사성을 반영 |\n",
    "| Contributions | $ \\cdot{} $ shallow log-linear 구조인 CBOW, skip-gram 모델 제시로 언어 모델 학습 효율 향상시킴 <br> $ \\cdot{} $ Hierarchical Softmax로 출력 예측 복잡도 $O \\big({} \\log_{2}(V) \\big){} $ 달성 <br> $ \\cdot{} $ 대규모 데이터 실험으로 연속형 단어 표현의 실용성 입증함 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93447f7",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185d940a",
   "metadata": {},
   "source": [
    "> ## **연구 배경(Motivation & Prior Work)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e67a00",
   "metadata": {},
   "source": [
    "#### **(1) 기존 연구 및 한계점** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 기존 NNLM(Neural Network Language Model)은 예측 토큰 시점 $ \\, t $, 고려하는 과거 단어의 개수(context window size)가 $ n $일 때 \n",
    "##### $ \\hspace{0.45cm} p(w_{t} | w_{t-n+1}, \\cdots{}, w_{t-1}) $를 모델링하지만, 샘플 당 계산 복잡도가 높음\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ RNNLM(RNN-Language Model)은 장기 의존성은 포착하지만 샘플 당 계산 복잡도가 높음\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 수십억 토큰 규모의 대규모 데이터셋에서 이러한 복잡도가 학습 병목으로 작용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45e8ff0d",
   "metadata": {},
   "source": [
    "#### **(2) 연구 목표** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 연산 효율이 높은 확률적 언어 모델을 설계\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 단어 간 의미적 유사성과 문법적 규칙성을 반영한 고품질 임베딩 벡터를 학습\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 대규모 데이터셋에서도 학습이 가능한 확장성(scalability)을 달성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2cb4f9",
   "metadata": {},
   "source": [
    "#### **(3) 제안된 방법론** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 문맥 단어(context word)의 임베딩 평균으로 중심 단어(center word)를 예측해 맥락 기반의 단어 의미를 효율적으로 학습하는 CBOW 모델 제안\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 그 중심 단어를 기반으로 주변 단어를 예측해 희귀 단어의 표현력을 강화하고 의미적, 통사적 관계를 포착하는 Skip-gram 모델 제안\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Hierarchical Softmax 기법을 활용해 대규모 어휘에서의 학습 효율성과 표현 품질을 동시에 향상"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81393004",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fb9cc7",
   "metadata": {},
   "source": [
    "> ## **방법론(Method)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d11e51",
   "metadata": {},
   "source": [
    "#### **(1) CBOW(Continuous Bag-of-Words)** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 주변 단어 집합 $ \\, \\mathcal{C}_{t} = \\{ w_{t-c}, \\cdots{}, w_{t-1}, w_{t+1}, \\cdots{}, w_{t+c} \\} $으로부터 중심 단어 $ w_{t} $를 예측할 확률을 최대화하는 것을 목표\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} \\displaystyle{} \\theta^{*} = \\arg\\max_{\\theta} \\sum_{t=1}^{T} \\log p(w_t | \\mathcal{C}_{t}; \\theta{}) $\n",
    "##### $ \\hspace{0.45cm} \\text{where } \\, \\mathcal{C}_{t} : \\text{input(context word) indices within window}, \\;\\; w_{t} : \\text{output(center word) index} $\n",
    "##### $ \\hspace{0.45cm} \\text{and } \\, T : \\text{token size}, \\;\\; c : \\text{window size} $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 주변 단어 임베딩의 평균을 통해 맥락 벡터 $ \\, \\bar{\\textbf{v}}_{t} $를 구성해 조건부 확률 $ \\, p(w_{t} | \\bar{\\textbf{v}}_{t}; \\theta{}) $을 (Hierarchical) softmax로 계산\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} p(w_t | \\mathcal{C}_{t}; \\theta{}) \\equiv{} p(w_{t} | \\bar{\\textbf{v}}_{t}) = \\displaystyle{} \\frac{\\exp{}\\big(\\textbf{u}_{w_{t}}^{\\top} \\bar{\\textbf{v}}_{t}\\big)}{\\displaystyle{} \\sum_{w' \\in{} \\mathcal{V}} \\exp{}\\big(\\textbf{u}_{w'}^{\\top} \\bar{\\textbf{v}}_{t}\\big)} $ \n",
    "##### $ \\hspace{0.45cm} \\text{where } \\, \\bar{\\textbf{v}}_{t} =  \\frac{1}{2c} \\sum_{\\substack{-c \\le j \\le c \\\\ j \\neq 0}} \\textbf{v}_{w_{t+j}} $\n",
    "##### $ \\hspace{0.45cm} \\text{and } \\, \\textbf{v}_{w_{t+j}} : \\text{input embedding vectors}, \\;\\; \\textbf{u}_{w_{t}} : \\text{output embedding vector} $\n",
    "##### $ \\hspace{0.45cm} \\text{and } \\, \\textbf{v}_{w} : \\text{final word embedding after training} $\n",
    "##### $ \\hspace{0.45cm} \\text{and } \\, \\mathcal{V} : \\text{vocabulary set}, \\;\\; |\\mathcal{V}|=V $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0633fda9",
   "metadata": {},
   "source": [
    "##### **(`PLUS`) 계층적 소프트맥스(Hierarchical Softmax)** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 각 단어를 리프 노드(leaf node)로하는 이진 트리에서 내부 노드 $ \\, n $ 마다 파라미터 벡터 $ \\mathbf{u}_{n} $를 두고 경로를 따라 이진 분류 확률을 곱하는 방법\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} p(w | \\mathbf{x} ) = \\displaystyle{} \\prod_{n \\in{} \\text{path}(w)} \\sigma{} \\big({} s(n,w) \\mathbf{u}_{n}^{\\top} \\mathbf{x} \\big){} $\n",
    "##### $ \\hspace{0.45cm} \\text{where } \\, \\text{path}(w) : \\text{root}\\to\\text{leaf}(w), \\;\\; s(n,w) \\in{} \\{ -1,+1 \\}, \\;\\; \\mathbf{u}_{n} \\in{} \\mathbb{R}^{D}, \\;\\; \\sigma{}(x) = \\frac{1}{1+\\exp{}(-x)} $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 허프만 코드(Huffman code)로 빈도 높은 단어의 경로를 짧게 하여 기대 연산량을 추가 절감함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df883541",
   "metadata": {},
   "source": [
    "#### **(2) Skip-gram** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 중심 단어 $ w_{t} $로부터 주변 단어 집합 $ \\, \\mathcal{C}_{t} = \\{ w_{t-c}, \\cdots{}, w_{t-1}, w_{t+1}, \\cdots{}, w_{t+c} \\} $를 예측을 목표\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} \\displaystyle{} \\theta^{*} = \\arg\\max_{\\theta{}} \\sum_{t=1}^{T} \\sum_{\\substack{-c \\le j \\le c \\\\ j \\neq 0}} \\log p(w_{t+j} \\mid w_{t}; \\theta{}) $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 중심 단어 임베딩 $ \\, \\textbf{v}_{w_t} $를 통해 주변 단어의 조건부 확률 $ \\, p(w_{t+j} | w_t; \\theta{}) $을 (Hierarchical) softmax로 계산\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} p(w_{t+j} | w_t; \\theta{}) = \\displaystyle{} \\frac{\\exp{}\\big(\\textbf{u}_{w_{t+j}}^{\\top} \\textbf{v}_{w_t}\\big)}{\\displaystyle{} \\sum_{w' \\in{} \\mathcal{V}} \\exp{}\\big(\\textbf{u}_{w'}^{\\top} \\textbf{v}_{w_t}\\big)} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828aa28c",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1fecaa",
   "metadata": {},
   "source": [
    "> ## **실험(Experiments)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12294fe2",
   "metadata": {},
   "source": [
    "#### **(1) 실험 설정** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 데이터셋\n",
    "| Item | Description |\n",
    "|------|--------------|\n",
    "| Training dataset | Google News $ + $ additional corpus |\n",
    "| Vocabulary size($ V $)| $ \\sim{}1\\text{M} \\, $ (most frequent words) |\n",
    "| token size($ T $) | $ \\sim{} 1.6\\text{B} \\, $ and $ 6\\text{B} $ |\n",
    "| Evaluation dataset | Semantic–Syntactic Word Relationship(Google analogy) |\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 모델\n",
    "| Item | Description |\n",
    "|------|--------------|\n",
    "| Model type | Continuous Bag-of-Words (CBOW) / Skip-gram |\n",
    "| Architecture | $ \\cdot{} $ Input: one-hot <br> $ \\cdot{} $ Projection: avg (CBOW) or single (Skip-gram), D-dim <br> $ \\cdot{} $ Output: Hierarchical Softmax |\n",
    "| Hyper parameter | $ \\cdot{} $ Embedding dimension($ D $) : $ 50 / 100 / 300 / 1000 $ <br> $ \\cdot{} $ Context window size($ c $) : CBOW $ c = 4 $, Skip-gram max $ c=10 $ (distance-weighted) <br> $ \\cdot{} $ Optimizer : SGD <br> $ \\cdot{} $ Learning rate : $ 0.025 $ (linearly decayed) |\n",
    "| Parameter size | $ \\cdot{} $ Parameter size : $ \\approx{} 2VD $ |\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 훈련 및 평가\n",
    "| Item | Description |\n",
    "|------|--------------|\n",
    "| Loss function | Negative log-likelihood |\n",
    "| Evaluation metric | Analogy exact-match accuracy |\n",
    "| Baseline models | Feed-forward NNLM, RNNLM |\n",
    "| Training hardware | Multi-threaded CPU (single-machine) & Distributed (DistBelief) |\n",
    "| Training time | ? | "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb99d20",
   "metadata": {},
   "source": [
    "#### **(2) 실험 결과** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 임베딩 차원 $ D $와 학습 데이터 크기(토큰 수 $ T $)를 함께 증가시킬 때, 단독으로 어느 한쪽만 늘리는 경우보다 품질 향상이 가장 큼\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Skip-gram 모델은 semantic 관계에서, CBOW는 syntactic 관계에서 상대적 강점을 가짐\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ NNLM 및 RNNLM 대비 은닉층이 없는 구조로 동일한 시간 내 더 많은 토큰을 처리하며, hierarchical softmax를 통해 학습 효율을 극대화함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88162db1",
   "metadata": {},
   "source": [
    "##### **(`PLUS`) 계산 복잡도(Computational Complexity; $ Q $) 비교** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} \\, Q_{\\text{NNLM}} = N D + N D H + H V  $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} \\, Q_{\\text{RNNLM}} = H^{2} + H V  $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} \\, Q_{\\text{CBOW}} = N D + D \\log_{2}{V} $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} \\, Q_{\\text{SG}} = C ( D + D \\log_{2}{V} ) $\n",
    "##### $ \\hspace{0.3cm} \\text{where } \\, N = 2c : \\text{context word size}, \\;\\; D : \\text{embedding dim}, \\;\\; H : \\text{hidden layer dim} $\n",
    "##### $ \\hspace{0.3cm} \\text{and } \\, V : \\text{vocabulary size}, \\;\\; C : \\text{max window radius} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46eb9f0e",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148f7ce3",
   "metadata": {},
   "source": [
    "> ## **결론(Conclusion)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3065a756",
   "metadata": {},
   "source": [
    "#### **(1)** CBOW는 평균 맥락 표현으로 계산을 단순화하여 효율을 확보"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630db6f0",
   "metadata": {},
   "source": [
    "#### **(2)** Skip-gram은 희소하지만 정보량 큰 주변 단서까지 개별 예측해 semantic 관계에 유리, CBOW는 평균 맥락으로 syntactic 관계에 유리함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9656f10c",
   "metadata": {},
   "source": [
    "#### **(3)** Hierarchical Softmax 도입으로 대규모 Vocabulary size로 인한 계산 병목을 완화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e342c4",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9d1ce0",
   "metadata": {},
   "source": [
    "> ## **부록(Appendix)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d22e575",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae8c3b6",
   "metadata": {},
   "source": [
    "> ## **비평(Commentary)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d980c44",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ce48cc",
   "metadata": {},
   "source": [
    "> ## **참고 문헌(Reference)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c453812",
   "metadata": {},
   "source": [
    "#### **(1) 관련 후속 논문** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ **[~]**\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ **[~]**\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ **[~]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c58bb3b",
   "metadata": {},
   "source": [
    "#### **(2) 기타 참고 자료** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ **[~]**\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ **[~]**\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ **[~]**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
