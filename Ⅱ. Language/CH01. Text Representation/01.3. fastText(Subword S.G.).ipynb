{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1053003b",
   "metadata": {},
   "source": [
    "# **CH01.3. fastText(Subword Skip-gram)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22532338",
   "metadata": {},
   "source": [
    "#### **`Paper Info`** : Enriching Word Vectors with Subword Information\n",
    "#### $ \\hspace{1.75cm} - $ Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov et al. (TACL/2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bf89a8",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97582d7",
   "metadata": {},
   "source": [
    "> ## **요약(Summary)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de97e7c4",
   "metadata": {},
   "source": [
    "| Item | Description |\n",
    "|------|------|\n",
    "| Research topic | Subword 기반 fastText 임베딩으로 Word-level 모델의 희귀어·OOV 및 형태 정보 손실 문제를 완화하는 방법 연구 |\n",
    "| Core idea | 각 단어를 문자 n-gram 벡터의 합으로 표현하고 SGNS 목표를 동일하게 최적화하여 파라미터 공유와 OOV 일반화를 달성 |\n",
    "| Key findings | $ \\cdot{} $ 다언어 word similarity·analogy에서 전반적 성능 향상 관찰 <br> $ \\cdot{} $ morphologically rich language에서 syntactic analogy 향상 폭이 큼 |\n",
    "| Contributions | $ \\cdot{} $ 간단한 n-gram 합 구조로 형태 정보 통합 <br> $ \\cdot{} $ 언어 비의존적이며 사전 형태소 분석기 없이 적용 가능 <br> $ \\cdot{} $ 희귀어·OOV에 강건하고 대규모 학습에 실용적 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca648d",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8814225",
   "metadata": {},
   "source": [
    "> ## **연구 배경(Motivation & Prior Work)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f88cd95",
   "metadata": {},
   "source": [
    "#### **(1) 기존 연구 및 한계점** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Word2Vec(CBOW·Skip-gram)과 GloVe는 단어를 atomic token으로 다뤄 내부 형태 정보를 놓침\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 형태소 기반 접근은 언어별 분석기 의존성과 파이프라인 복잡도를 증가시켜 범용 적용이 어려움\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ character-level RNN·CNN·BPE는 문자 조합이나 subword 빈도를 학습하나 단어 표현과의 직접적 합성 규칙이 단순하지 않음\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 결과적으로 희귀어·OOV에서 의미 표현 품질 저하 및 파라미터 데이터 효율성 저하가 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf352385",
   "metadata": {},
   "source": [
    "#### **(2) 연구 목표** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 형태가 풍부한 언어에서도 견고한 단어 표현을 제공하여 희귀어·OOV 성능 저하를 최소화\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 언어·도메인 비의존적으로 대규모 코퍼스에 효율적으로 적용 가능한 간단한 임베딩 방법을 제시\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ word similarity·analogy·language modeling 등 다양한 벤치마크에서 일관된 개선을 보이는 범용 임베딩 확보"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e0f271",
   "metadata": {},
   "source": [
    "#### **(3) 제안된 방법론** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Skip-gram with Negative Sampling의 스코어를 유지하되 입력 벡터를 단어의 문자 n-gram 벡터 합으로 정의하여 파라미터 공유와 OOV 일반화를 동시에 달성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e0d32",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbdcdb3",
   "metadata": {},
   "source": [
    "> ## **방법론(Method)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf2e919",
   "metadata": {},
   "source": [
    "#### **(1) SGNS 기본 구조** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 중심 단어 $ w_{t} $와 윈도우 내 문맥 $ w_{c} $에 대해 로지스틱 이진 분류 손실을 최소화\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} \\displaystyle{} \\mathcal{L} = \\sum_{t=1}^{T} \\sum_{c \\in{} C_{t}} \\Big\\{ -\\log{}\\sigma\\big(s(w_{t}, w_{c})\\big) - \\sum_{n \\in{} N_{t,c}} \\log{}\\sigma\\big(-s(w_{t}, n)\\big) \\Big\\} $\n",
    "##### $ \\hspace{0.45cm} \\text{ where } \\, \\sigma{}(\\cdot{}) : \\text{sigmoid} , \\; C_{t} : \\text{context indices around } t , \\; N_{t,c} : \\text{negative samples set} $\n",
    "##### $ \\hspace{0.45cm} \\text{ and } \\, s(w_{i}, w_{o}) = \\mathbf{u}_{w_{i}}^{\\top} \\mathbf{v}_{w_{o}} , \\; \\mathbf{u}_{w} , \\mathbf{v}_{w} \\in{} \\mathbb{R}^{d} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f01efa9",
   "metadata": {},
   "source": [
    "#### **(2) Subword 합성 스코어** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 각 단어 $ w $에 경계기호를 포함해 문자 n-gram 집합 $ G_{w} $를 생성하고 각 n-gram에 파라미터 $ \\mathbf{z}_{g} \\in{} \\mathbb{R}^{d} $를 둠\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} \\mathbf{u}_{w} = \\displaystyle{} \\sum_{g \\in{} G_{w}} \\mathbf{z}_{g} \\;\\; \\Rightarrow{} \\;\\; s(w,c) = \\mathbf{u}_{w}^{\\top} \\mathbf{v}_{c} = \\sum_{g \\in{} G_{w}} \\mathbf{z}_{g}^{\\top} \\mathbf{v}_{c} $\n",
    "##### $ \\hspace{0.45cm} \\text{ where } \\, G_{w} : \\text{character n-gram indices of } w , \\; n \\in{} \\{3,\\ldots{},6\\} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb171f",
   "metadata": {},
   "source": [
    "##### **(`PLUS`) OOV 표현** : 학습 중 미등장 단어라도 구성 n-gram이 학습되었으면 $ \\mathbf{u}_{w} $를 합성 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a1522",
   "metadata": {},
   "source": [
    "#### **(3) 경사 유도 및 파라미터 공유** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ $ s = \\mathbf{u}_{w_{t}}^{\\top} \\mathbf{v}_{w} $일 때 $ -\\log{}\\sigma(s) $의 미분은 $ \\partial L / \\partial s = \\sigma(-s) $\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} \\partial L / \\partial \\mathbf{u}_{w_{t}} = \\big(1 - \\sigma(s)\\big) \\mathbf{v}_{w} , \\;\\; \\partial L / \\partial \\mathbf{v}_{w} = \\big(1 - \\sigma(s)\\big) \\mathbf{u}_{w_{t}} $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ $ \\mathbf{u}_{w_{t}} = \\sum_{g \\in{} G_{w_{t}}} \\mathbf{z}_{g} $ 이므로 연쇄법칙으로 $ \\partial L / \\partial \\mathbf{z}_{g} = \\partial L / \\partial \\mathbf{u}_{w_{t}} $ for $ g \\in{} G_{w_{t}} $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 동일 단어 내 모든 n-gram이 동일한 그라디언트를 공유하여 희귀 형태들 간 파라미터 공유가 자연히 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebde0c6a",
   "metadata": {},
   "source": [
    "#### **(4) 해시 버킷팅으로 메모리 상한화** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ n-gram을 FNV-1a 등 해시로 $ K $ 버킷에 사상하고 충돌은 파라미터 공유로 처리\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} K \\approx{} 2 \\times 10^{6} \\;\\; \\text{ where } \\, K : \\text{number of hash buckets controlling memory} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a77c530f",
   "metadata": {},
   "source": [
    "##### **(`PLUS`) 직관적 효과** : 접사·활용·복합어 등 형태 단위가 벡터에 반영되어 syntactic relation과 희귀어 유사도 개선을 유도"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1996ceb",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c094e5",
   "metadata": {},
   "source": [
    "> ## **실험(Experiments)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513debef",
   "metadata": {},
   "source": [
    "#### **(1) 실험 설정** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 데이터셋\n",
    "| Item | Description |\n",
    "|------|--------------|\n",
    "| Training dataset | Wikipedia dumps (9 langs: AR, CS, DE, EN, ES, FR, IT, RO, RU) |\n",
    "| Data(Sentence;row) size | ? |\n",
    "| Vocabulary(token) | ? |\n",
    "| Evaluation dataset | Word similarity (WS353, RW, multi-lingual), Analogy (semantic/syntactic), LM (CS, DE, ES, FR, RU ~1M) |\n",
    "| Evaluation size | ? |\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 모델 및 훈련\n",
    "| Item | Description |\n",
    "|------|--------------|\n",
    "| Model type | Skip-gram / Subword Skip-gram (fastText) |\n",
    "| Hidden layer | None |\n",
    "| Embedding dimension | 300 |\n",
    "| Context window size | 1–5 |\n",
    "| Negatives | 5 |\n",
    "| Subsampling | 1e-4 |\n",
    "| Optimizer | SGD + linear decay |\n",
    "| Epochs | 5 |\n",
    "| N-gram length | 3–6 |\n",
    "| Hash buckets | 2e6 |\n",
    "| Training hardware | ? |\n",
    "| Training time | ? |\n",
    "| Baseline models | Skip-gram, CBOW, morphology-based baselines |\n",
    "| Evaluation metric | Spearman corr., Analogy accuracy, Perplexity |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4653c130",
   "metadata": {},
   "source": [
    "#### **(2) 분석 및 결과** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ word similarity에서 subword 모델이 대부분의 언어와 벤치마크에서 sg·cbow 대비 우수하며 EN WS353만 근소 열위 경향\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ EN RW 등 희귀어 비중이 높은 셋에서 개선 폭이 큼\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ analogy에서는 syntactic task에서 큰 향상이며 semantic task에서는 제한적 이득\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ LSTM language model 초기화에 사용 시 모든 언어에서 perplexity가 sg 초기화보다 낮음\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 데이터가 매우 작아도 subword는 cbow full-data에 준하거나 상회하는 경향을 보여 데이터 효율성이 높음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5f5069",
   "metadata": {},
   "source": [
    "##### **(`PLUS`) 효율성** : SGNS 대비 처리량이 약 $ \\sim{} 1.5\\times $ 느리지만 여전히 대규모 학습에 실용적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5acb6e2b",
   "metadata": {},
   "source": [
    "#### **(3) 결과 해석** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ n-gram 합으로 파라미터가 공유되어 희귀형태에서 분산 추정 분산이 감소하고 OOV에서도 의미 표현이 가능\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 형태가 풍부한 언어에서 접사·활용·복합어 정보가 임베딩에 반영되어 syntactic relation을 더 잘 포착\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ semantic analogy는 subword 단위만으로는 world knowledge 관계를 충분히 보강하지 못해 이득이 제한적"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1eca48",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c98e64",
   "metadata": {},
   "source": [
    "> ## **결론(Conclusion)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91814d7c",
   "metadata": {},
   "source": [
    "#### **(1)** Skip-gram의 스코어를 유지한 채 입력을 subword 합으로 치환하는 간단한 설계로 희귀어·OOV 및 형태 정보 손실 문제를 완화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21040626",
   "metadata": {},
   "source": [
    "#### **(2)** 언어 비의존적이며 형태소 분석기 없이도 적용 가능해 실무 파이프라인 단순성과 확장성을 제공"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7097ee48",
   "metadata": {},
   "source": [
    "#### **(3)** morphologically rich language에서 이득이 특히 크며 LM 초기화 등 다운스트림에서도 일관된 개선을 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb4dbdd",
   "metadata": {},
   "source": [
    "#### **(4)** 최적 n-gram 길이 선택은 데이터·언어·태스크에 따라 달라지는 열린 문제로 후속 연구의 여지가 큼"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd8e200",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f1deaf",
   "metadata": {},
   "source": [
    "> ## **부록(Appendix)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa754d63",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae3a35",
   "metadata": {},
   "source": [
    "> ## **비평(Commentary)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb066eec",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0473fb9b",
   "metadata": {},
   "source": [
    "> ## **참고 문헌(Reference)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6567a6",
   "metadata": {},
   "source": [
    "#### **(1) 관련 후속 논문** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ebade0",
   "metadata": {},
   "source": [
    "#### **(2) 기타 참고 자료** :\n",
    "#### - 블로그, 연구실 발표 자료 등등"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
