{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1053003b",
   "metadata": {},
   "source": [
    "# **CH01.2. GloVe**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22532338",
   "metadata": {},
   "source": [
    "#### **`Paper Info`** : GloVe: Global Vectors for Word Representation\n",
    "##### $ \\hspace{1.75cm} - $ Jeffrey Pennington, Richard Socher, Christopher D. Manning et al. (EMNLP/2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bf89a8",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97582d7",
   "metadata": {},
   "source": [
    "> ## **요약(Summary)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd42de9",
   "metadata": {},
   "source": [
    "| Item | Description |\n",
    "|------|-------------|\n",
    "| Research topic | 전역 co-occurrence 통계를 활용한 log-bilinear 회귀로 단어 분산 표현을 학습하는 GloVe 모델 제안 |\n",
    "| Key findings | $ \\cdot{} $ 확률 비율 ratio 기반 직관으로 linear substructure를 유도해 analogy에 강점 확보 <br> $ \\cdot{} $ 가중 최소제곱 회귀와 weighting function으로 희귀 단어와 초빈 단어를 균형 있게 학습 <br> $ \\cdot{} $ Word analogy, Word similarity, NER에서 기존 count 기반과 prediction 기반 대비 우수 성능 |\n",
    "| Contributions | $ \\cdot{} $ count based와 predictive 접근을 이론적으로 연결하는 목적함수 제안 <br> $ \\cdot{} $ 교환 대칭 word context를 보존하는 bias 포함 log bilinear 모델화 <br> $ \\cdot{} $ 대규모 말뭉치에서 효율적 학습과 재현 가능한 설정 제시 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca648d",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8814225",
   "metadata": {},
   "source": [
    "> ## **연구 배경(Motivation & Prior Work)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24dbda",
   "metadata": {},
   "source": [
    "#### **(1) 기존 연구 및 한계점** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ LSA(Latent Semantic Analysis) 등 count-based 방법은 전역 통계를 활용하지만 단어 간 선형 의미 관계를 반영하지 못함\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Skip-gram, CBOW 등 predict-based 방법은 의미 관계를 잘 학습하지만 전역 통계 정보 활용이 제한적임"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c05c3d7",
   "metadata": {},
   "source": [
    "##### **(`PLUS`) 선형 의미 관계(Linear Semantic Relationship)** : 단어들 간의 의미적 관계가 벡터 공간에서 선형 연산(덧셈, 뺄셈)으로 표현될 수 있는 관계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e944931",
   "metadata": {},
   "source": [
    "#### **(2) 연구 목표** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 전역 통계 활용 효율성과 예측 기반 선형 의미 구조를 동시에 달성\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 단어 의미 차이를 조건부 동시출현 확률의 비율로 모델링\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 단순하고 해석 가능한 목적식을 통해 의미적 규칙성을 반영"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde8c355",
   "metadata": {},
   "source": [
    "#### **(3) 제안된 방법론** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 단어 쌍의 log co-occurrence를 임베딩 내적과 bias의 합으로 근사함\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ weighted least squares 목적함수를 사용해 빈도 불균형을 완화함\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 학습된 target과 context 벡터의 합을 최종 단어 임베딩으로 구성함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e0d32",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbdcdb3",
   "metadata": {},
   "source": [
    "> ## **방법론(Method)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ced865b",
   "metadata": {},
   "source": [
    "#### **(0) Co-occurrence** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Co-occurrence matrix : corpus 내에서 단어 쌍 $ \\, (w_{i},w_{j}) $가 같은 (문맥) 윈도우 안에서 함께 등장한 빈도(횟수)를 집계한 행렬 \n",
    "##### $ \\hspace{0.3cm} \\Rightarrow{} X_{ij} := \\displaystyle{} \\sum_{t=1}^{T} \\sum_{\\substack{-c \\le{} d \\le{} c \\\\ d \\neq{} 0}} \\mathbb{I}\\{w_{t}=i, w_{t+d}=j\\} $\n",
    "##### $ \\hspace{0.45cm} \\text{where } \\, \\mathbb{I}\\{\\cdot{}\\} : \\text{indicator function}, \\;\\; c : \\text{window radius}, \\;\\; T : \\text{token length} $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Co-occurence (conditional) probability : 중심 단어 $ w_{i} $가 주어졌을 때 문맥 단어 $ w_{j} $가 같은 윈도우 내에 등장할 확률\n",
    "##### $ \\hspace{0.3cm} \\Rightarrow{} P(w_{j}|w_{i}) = \\displaystyle{} \\frac{X_{ij}}{X_{i}} \\;\\; \\text{ where } \\, X_{i} = \\displaystyle{} \\sum^{V}_{k=1} X_{ik} $ \n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Co-occurrence ratio : 두 단어 $ w_{i} $와 $ w_{j} $가 특정 단어 $ w_{k} $에 대해 갖는 co-occurrence probability의 비율\n",
    "##### $ \\hspace{0.3cm} \\Rightarrow{} \\displaystyle{} \\frac{P(w_{k}\\mid{}w_{i})}{P(w_{k}\\mid{}w_{j})} = \\frac{X_{ik}/X_{i}}{X_{jk}/X_{j}} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbfca931",
   "metadata": {},
   "source": [
    "##### **(`PLUS`)** Co-occurrence ratio는 임베딩 공간에서 벡터 차이를 표현하기 위해  $ F((\\mathbf{w}_{i}-\\mathbf{w}_{j})^{\\top{}}\\tilde{\\mathbf{w}}_{k}) $ 형태로 매핑됨\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} \\displaystyle{} \\frac{P(w_{k}\\mid{}w_{i})}{P(w_{k}\\mid{}w_{j})} = \\exp{}\\bigg({}( \\mathbf{w}_{i}-\\mathbf{w}_{j} )^{\\top{}}\\tilde{\\mathbf{w}}_{k}\\bigg){} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63208eb6",
   "metadata": {},
   "source": [
    "#### **(1) GloVe(Global Vectors for Word Representation)** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Log co-occurence conditional probability $ \\, \\log{} \\big({} P(w_{j}|w_{i}) \\big){} $를 log linear 모델로 가정(근사)\n",
    "##### $ \\hspace{0.3cm} \\Rightarrow{} \\log{} \\big({} P(w_{j}|w_{i}) \\big){} \\approx{} \\mathbf{w}_{i}^{\\top{}}\\tilde{\\mathbf{w}}_{j} + b_{i} + \\tilde{b}_{j} - \\log{} Z_{i} $\n",
    "##### $ \\hspace{0.45cm} \\text{where } \\, X_{i} = \\displaystyle{} \\sum^{V}_{k=1} X_{ik}, \\;\\; \\mathbf{w}_{i},\\tilde{\\mathbf{w}}_{j}\\in{}\\mathbb{R}^{d} : \\text{target, context embeddings} $\n",
    "##### $ \\hspace{0.45cm} \\text{and } \\, b_{i},\\tilde{b}_{j}\\in{}\\mathbb{R} : \\text{bias terms}, \\;\\; Z_{i}: \\text{normalizer} $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Weighted Least Squares Error 문제로 손실 함수를 정의\n",
    "##### $ \\hspace{0.3cm} \\Rightarrow{} J^{*} \\approx{} \\displaystyle{} \\argmin_{\\theta{}} \\sum_{i=1}^{V}\\sum_{j=1}^{V} f(X_{ij})\\Big({}\\mathbf{w}_{i}^{\\top{}}\\tilde{\\mathbf{w}}_{j} + b_{i} + \\tilde{b}_{j} - \\log X_{ij}\\Big){}^{2} $\n",
    "##### $ \\hspace{0.45cm} \\text{where } \\, f(x)=\\begin{cases}(x/x_{\\max{}})^{\\alpha{}} & x<x_{\\max{}} \\\\ 1 & \\text{otherwise}\\end{cases} \\; : \\text{weighted function} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f6045a",
   "metadata": {},
   "source": [
    "##### **(`WHY?`)**\n",
    "##### $ \\hspace{0.3cm} \\cdot{} $ log-linear 모델로 조건부 확률의 로그를 내적과 바이어스로 근사함을 가정\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} \\log{} \\big({} P(w_{j}\\mid{}w_{i}) \\big){} = \\log{} (\\frac{X_{ij}}{X_{i}})\\approx{} \\mathbf{w}_{i}^{\\top{}}\\tilde{\\mathbf{w}}_{j} + b_{i} + \\tilde{b}_{j} - \\log{} Z_{i} $\n",
    "##### $ \\hspace{0.45cm} \\Leftrightarrow{} \\log{} (X_{ij}) - \\log{} (X_{i}) \\approx{} \\mathbf{w}_{i}^{\\top{}}\\tilde{\\mathbf{w}}_{j} + b_{i} + \\tilde{b}_{j} - \\log{} (Z_{i}) $\n",
    "##### $ \\hspace{0.3cm} \\cdot{} $ 이항하여 $ \\, \\log{} (X_{ij}) $를 target variable로 하는 선형 회귀 형태로 변환\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} \\log{} (X_{ij}) \\approx{} \\mathbf{w}_{i}^{\\top{}}\\tilde{\\mathbf{w}}_{j} + b_{i} + \\tilde{b}_{j} + \\log{} (X_{i}) - \\log{} (Z_{i}) $\n",
    "##### $ \\hspace{0.3cm} \\cdot{} \\, \\log{} Z_{i} $와 $ \\log{} X_{i} $항은 단어 $ i $에만 의존하므로 바이어스 $ b_{i} $로 흡수\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} \\log{} (X_{ij}) = \\mathbf{w}_{i}^{\\top{}}\\tilde{\\mathbf{w}}_{j} + b_{i}^{'} + \\tilde{b}_{j} \\;\\; \\text{ where } \\, b_{i}^{'} = b_{i} + \\log{} (X_{i}) - \\log{} (Z_{i}) $\n",
    "##### $ \\hspace{0.3cm} \\cdot{} $ Weighted Least Squares Error 정의\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} \\text{MSE} : \\displaystyle{} f(X_{ij}) \\cdot{} \\epsilon{}^{2} \\;\\; \\text{ where } \\,  \\epsilon{} := \\big({} \\mathbf{w}_{i}^{\\top{}}\\tilde{\\mathbf{w}}_{j} + b_{i}^{'} + \\tilde{b}_{j} \\big){} - \\log{}(X_{ij}) $\n",
    "##### $ \\hspace{0.45cm} \\Leftrightarrow{} J^{*} \\approx{} \\displaystyle{} \\argmin_{\\theta{}} \\sum_{i=1}^{V}\\sum_{j=1}^{V} f(X_{ij}) \\cdot{} \\epsilon{}^{2} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a230662",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c094e5",
   "metadata": {},
   "source": [
    "> ## **실험(Experiments)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c43094",
   "metadata": {},
   "source": [
    "#### **(1) 실험 설정** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 데이터셋\n",
    "| Item | Description |\n",
    "|------|--------------|\n",
    "| Training dataset | Wikipedia 2014 + Gigaword 5 + Common Crawl |\n",
    "| Vocabulary size($V$) | $ 400\\text{K} $ / $ 1.9\\text{M} $ |\n",
    "| token size($T$) | $ 6\\text{B} $ / $ 42\\text{B} $ |\n",
    "| Evaluation dataset | Google analogy, MSR analogy, WordSim353, MEN |\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 모델\n",
    "| Item | Description |\n",
    "|------|--------------|\n",
    "| Model type | Global log-bilinear regression (GloVe) |\n",
    "| Architecture | target, context embeddings + bias |\n",
    "| Parameter size | $ \\approx{} 2VD + 2V$ |\n",
    "| Hyper parameter | $ \\cdot{} $ embedding dimension($ D $) $ = 50/100/200/300$ <br> $ \\cdot{} $ window size($ c $) = 10 <br> $ \\cdot{} \\, x_{\\max}=100$, $\\alpha=0.75$ |\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 훈련 및 평가\n",
    "| Item | Description |\n",
    "|------|--------------|\n",
    "| Loss function | Weighted least squares on $\\log{} X_{ij}$ |\n",
    "| Evaluation metric | Analogy accuracy, Spearman correlation |\n",
    "| Baseline models | Skip-gram, CBOW, LSA (SVD-based) |\n",
    "| Training hardware | CPU multi-threaded |\n",
    "| Training time | ? |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10207de3",
   "metadata": {},
   "source": [
    "#### **(2) 실험 결과** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Word analogy에서 semantic·syntactic 관계 모두에서 높은 정확도 달성 ($ 71.7\\% $ at $ 6\\text{B}, \\; 75\\% $ at $ 42\\text{B} $)\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Word similarity에서 Spearman 상관 WS-353($ 75.9 $), MC($ 83.6 $), RG($ 82.9 $)로 강한 상관성을 보임\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ NER(CoNLL-2003) 실험에서 CRF pipeline의 F1 향상($88.3\\%$)을 달성함\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 임베딩 차원이 증가함에 따라 성능은 향상되나 약 $ 200D $ 이후 수익 체감이 발생함\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 작은 윈도우는 syntactic 관계에, 큰 윈도우는 semantic 관계에 유리함\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 대규모 코퍼스일수록 전역 의미 정보가 강화되어 semantic accuracy가 상승함\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Skip-gram, CBOW 대비 시간–정확도 효율성이 높음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1eca48",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c98e64",
   "metadata": {},
   "source": [
    "> ## **결론(Conclusion)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba03ff5",
   "metadata": {},
   "source": [
    "#### **(1)** GloVe는 전역 co-occurrence ratio를 내적 선형식으로 모델링하여 의미 관계를 공간적으로 표현함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e38d5c",
   "metadata": {},
   "source": [
    "#### **(2)** weighted least squares 목적함수와 weighting function으로 고빈도·저빈도 단어의 영향을 균형화함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b0a3c7",
   "metadata": {},
   "source": [
    "#### **(3)** count-based와 predict-based 접근의 장점을 결합하여 대규모 코퍼스에서도 효율성과 성능을 모두 달성함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd8e200",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f1deaf",
   "metadata": {},
   "source": [
    "> ## **부록(Appendix)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa754d63",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae3a35",
   "metadata": {},
   "source": [
    "> ## **비평(Commentary)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb066eec",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0473fb9b",
   "metadata": {},
   "source": [
    "> ## **참고 문헌(Reference)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df1e1bc",
   "metadata": {},
   "source": [
    "#### **(1) 관련 후속 논문** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ **[~]**\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ **[~]**\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ **[~]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc4b681",
   "metadata": {},
   "source": [
    "#### **(2) 기타 참고 자료** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ **[~]**\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ **[~]**\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ **[~]**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
