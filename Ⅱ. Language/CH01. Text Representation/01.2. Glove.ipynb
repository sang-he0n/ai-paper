{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1053003b",
   "metadata": {},
   "source": [
    "# **CH01.2. GloVe**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22532338",
   "metadata": {},
   "source": [
    "#### **`Paper Info`** : GloVe: Global Vectors for Word Representation\n",
    "#### $ \\hspace{1.75cm} - $ Jeffrey Pennington, Richard Socher, Christopher D. Manning et al. (EMNLP/2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94bf89a8",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97582d7",
   "metadata": {},
   "source": [
    "> ## **요약(Summary)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e928819",
   "metadata": {},
   "source": [
    "| Item | Description |\n",
    "|------|-------------|\n",
    "| Research topic | 전역 word-word co-occurrence 통계를 log-bilinear 회귀로 근사하여 단어 임베딩을 학습하는 GloVe 모델 제안 |\n",
    "| Key findings | $ \\cdot{} $ 단어 의미 구분은 probe 단어 $ k $에 대한 조건부 동시출현 확률의 비율 $ P(k|i)/P(k|j) $로 설명 가능하며, 해당 비율의 로그가 임베딩 내적과 선형적으로 대응함 <br> $ \\cdot{} $ weighted least squares 목적함수와 weighting function $ f(x) $ 설계로 희귀·초빈 단어의 영향을 균형화함 <br> $ \\cdot{} $ Word analogy, similarity, NER 등에서 Word2Vec 대비 경쟁력 있는 성능을 보임 |\n",
    "| Contributions | $ \\cdot{} $ co-occurrence 기반 통계적 정보와 예측 기반 모델의 장점을 결합함 <br> $ \\cdot{} $ 로그-회귀 기반의 단순하면서도 해석 가능한 학습 구조를 제시함 <br> $ \\cdot{} $ 대규모 코퍼스에서도 효율적으로 학습 가능한 확장성을 입증함 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ca648d",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8814225",
   "metadata": {},
   "source": [
    "> ## **연구 배경(Motivation & Prior Work)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb24dbda",
   "metadata": {},
   "source": [
    "#### **(1) 기존 연구 및 한계점** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ LSA 등 count-based 방법은 전역 통계를 활용하지만 단어 간 선형 의미 관계를 반영하지 못함\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Skip-gram, CBOW 등 predict-based 방법은 의미 관계를 잘 학습하지만 전역 통계 정보 활용이 제한적임\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 두 접근의 장점을 통합한 이론적·실용적 학습 틀이 부재함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e944931",
   "metadata": {},
   "source": [
    "#### **(2) 연구 목표** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 전역 통계 활용 효율성과 예측 기반 선형 의미 구조를 동시에 달성\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 단어 의미 차이를 조건부 동시출현 확률의 비율로 모델링\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 단순하고 해석 가능한 목적식을 통해 의미적 규칙성을 반영"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde8c355",
   "metadata": {},
   "source": [
    "#### **(3) 제안된 방법론** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 단어 쌍의 로그 co-occurrence를 임베딩 내적과 bias의 합으로 근사함\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ weighted least squares 목적함수를 사용해 빈도 불균형을 완화함\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 학습된 target과 context 벡터의 합을 최종 단어 임베딩으로 구성함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4e0d32",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbdcdb3",
   "metadata": {},
   "source": [
    "> ## **방법론(Method)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ced865b",
   "metadata": {},
   "source": [
    "#### **(0) co-occurrence ratio** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 코퍼스의 동시출현 행렬 $ X \\in{} \\mathbb{R}^{V \\times V} $에서 각 원소 $ X_{ij} $는 단어 $ i $ 주변 윈도우 내에서 단어 $ j $가 등장한 횟수를 의미함\n",
    "##### $ \\hspace{0.45cm} \\Rightarrow{} X_{ij} = \\sum_{t} \\mathbb{I}\\{ w_{t}=i,\\, w_{t+j}=j \\} \\;\\; \\text{ where } \\, \\mathbb{I}\\{\\cdot\\}: \\text{indicator function within window} $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 조건부 확률을 $ P_{ij}=P(j|i)=X_{ij}/X_{i} $로 정의함\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 단어 의미의 차이는 probe 단어 $ k $에 대한 조건부 확률의 비율 $ P(k|i)/P(k|j) $의 로그로 표현할 수 있음\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} (\\mathbf{w}_{i}-\\mathbf{w}_{j})^{\\top}\\tilde{\\mathbf{w}}_{k} = \\log{}{\\frac{P_{ik}}{P_{jk}}} \\;\\; \\text{ where } \\, P_{ik}=X_{ik}/X_{i}, \\;\\; P_{jk}=X_{jk}/X_{j}, \\;\\; \\mathbf{w}_{i},\\tilde{\\mathbf{w}}_{k} \\in{} \\mathbb{R}^{d} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24688f9",
   "metadata": {},
   "source": [
    "#### **(1) GloVe(Global Vectors for Word Representation)** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 단어 $ i $가 주어졌을 때 단어 $ j $가 등장할 확률 $ P(j|i) $를 근사하도록 임베딩을 학습하는 것을 목표\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} \\displaystyle{} \\theta^{*} = \\arg\\max_{\\theta{}} \\sum_{i=1}^{V}\\sum_{j=1}^{V} \\log p_{\\theta{}}(j|i) $\n",
    "##### $ \\hspace{0.45cm} \\text{where } \\, V : \\text{vocabulary size}, \\;\\; \\theta{} = \\{ \\mathbf{w}_{i}, \\tilde{\\mathbf{w}}_{j}, b_{i}, \\tilde{b}_{j} \\} $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 조건부 확률 $ p_{\\theta{}}(j|i) $는 단어 벡터 내적과 bias를 이용한 softmax 형태로 정의됨\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} p_{\\theta{}}(j|i) = \\displaystyle{} \\frac{\\exp{}\\big(\\mathbf{w}_{i}^{\\top}\\tilde{\\mathbf{w}}_{j} + b_{i} + \\tilde{b}_{j}\\big)}{\\displaystyle{} \\sum_{j'\\in{}\\mathcal{V}} \\exp{}\\big(\\mathbf{w}_{i}^{\\top}\\tilde{\\mathbf{w}}_{j'} + b_{i} + \\tilde{b}_{j'}\\big)} $\n",
    "##### $ \\hspace{0.45cm} \\text{where } \\, \\mathbf{w}_{i}, \\tilde{\\mathbf{w}}_{j} \\in{} \\mathbb{R}^{d} : \\text{target, context embeddings}, \\;\\; b_{i}, \\tilde{b}_{j} \\in{} \\mathbb{R} : \\text{bias terms} $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 실제 코퍼스의 조건부 확률 $ P(j|i)=X_{ij}/X_{i} $를 모델 확률 $ p_{\\theta{}}(j|i) $로 근사하도록 학습하며, 두 확률의 로그 비율 관계는 다음과 같음\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} (\\mathbf{w}_{i}-\\mathbf{w}_{j})^{\\top}\\tilde{\\mathbf{w}}_{k} = \\log{}{\\frac{P(k|i)}{P(k|j)}} \\;\\; \\text{ where } \\, P(k|i)=X_{ik}/X_{i}, \\;\\; X_{i}=\\sum_{m}X_{im} $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 이를 모든 단어쌍 $(i,j)$에 대해 근사하기 위해, 로그 동시출현 빈도를 내적과 bias의 합으로 표현하는 로그-선형 회귀식을 사용함\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} \\log{}{X_{ij}} \\approx \\mathbf{w}_{i}^{\\top}\\tilde{\\mathbf{w}}_{j} + b_{i} + \\tilde{b}_{j} $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 학습은 관측된 $ X_{ij} $에 대해 weighted least squares 손실을 최소화하도록 수행됨\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} J^{*} = \\arg\\min_{\\theta{}} \\sum_{i=1}^{V}\\sum_{j=1}^{V} f(X_{ij})\\big(\\mathbf{w}_{i}^{\\top}\\tilde{\\mathbf{w}}_{j} + b_{i} + \\tilde{b}_{j} - \\log{}{X_{ij}}\\big)^{2} $\n",
    "##### $ \\hspace{0.45cm} \\text{where } \\, f(x)=(x/x_{\\max})^{\\alpha} \\text{ if } x<x_{\\max} \\text{ otherwise } 1, \\;\\; x_{\\max}=100, \\;\\; \\alpha=3/4 $\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 최종 단어 임베딩은 target 벡터와 context 벡터의 합으로 구성함\n",
    "##### $ \\hspace{0.15cm} \\Rightarrow{} \\mathbf{v}_{i} = \\mathbf{w}_{i} + \\tilde{\\mathbf{w}}_{i} $\n",
    "##### $ \\hspace{0.45cm} \\text{where } \\, \\mathbf{v}_{i} \\in{} \\mathbb{R}^{d} : \\text{final embedding for word } i $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a230662",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c094e5",
   "metadata": {},
   "source": [
    "> ## **실험(Experiments)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c43094",
   "metadata": {},
   "source": [
    "#### **(1) 실험 설정** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 데이터셋\n",
    "| Item | Description |\n",
    "|------|--------------|\n",
    "| Training dataset | Wikipedia 2014 + Gigaword 5 (6B tokens) / Common Crawl (42B tokens) |\n",
    "| Vocabulary size($V$) | 400K (default), up to 1.9M |\n",
    "| token size($T$) | 6B / 42B |\n",
    "| Evaluation dataset | Google analogy, MSR analogy, WordSim353, MEN |\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 모델\n",
    "| Item | Description |\n",
    "|------|--------------|\n",
    "| Model type | Global log-bilinear regression (GloVe) |\n",
    "| Architecture | target, context embeddings + bias |\n",
    "| Parameter size | $\\approx 2Vd + 2V$ |\n",
    "| Hyper parameter | $d=50/100/200/300$, window size = 10, $x_{\\max}=100$, $\\alpha=0.75$ |\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 훈련 및 평가\n",
    "| Item | Description |\n",
    "|------|--------------|\n",
    "| Loss function | Weighted least squares on $\\log X_{ij}$ |\n",
    "| Evaluation metric | Analogy accuracy, Spearman correlation |\n",
    "| Baseline models | Skip-gram, CBOW, LSA (SVD-based) |\n",
    "| Training hardware | CPU multi-threaded |\n",
    "| Training time | ? |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10207de3",
   "metadata": {},
   "source": [
    "#### **(2) 실험 결과** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Word analogy에서 semantic·syntactic 관계 모두에서 높은 정확도 달성 ($71.7\\%$ at 6B, $75.0\\%$ at 42B)\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Word similarity에서 Spearman 상관 WS-353(75.9), MC(83.6), RG(82.9)로 강한 상관성을 보임\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ NER(CoNLL-2003) 실험에서 CRF pipeline의 F1 향상($88.3\\%$)을 달성함\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 임베딩 차원이 증가함에 따라 성능은 향상되나 약 200D 이후 수익 체감이 발생함\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 작은 윈도우는 syntactic 관계에, 큰 윈도우는 semantic 관계에 유리함\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ 대규모 코퍼스일수록 전역 의미 정보가 강화되어 semantic accuracy가 상승함\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ Skip-gram, CBOW 대비 시간–정확도 효율성이 높음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1eca48",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1c98e64",
   "metadata": {},
   "source": [
    "> ## **결론(Conclusion)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba03ff5",
   "metadata": {},
   "source": [
    "#### **(1)** GloVe는 전역 co-occurrence ratio를 내적 선형식으로 모델링하여 의미 관계를 공간적으로 표현함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e38d5c",
   "metadata": {},
   "source": [
    "#### **(2)** weighted least squares 목적함수와 weighting function으로 고빈도·저빈도 단어의 영향을 균형화함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b0a3c7",
   "metadata": {},
   "source": [
    "#### **(3)** count-based와 predict-based 접근의 장점을 결합하여 대규모 코퍼스에서도 효율성과 성능을 모두 달성함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd8e200",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f1deaf",
   "metadata": {},
   "source": [
    "> ## **부록(Appendix)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa754d63",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ae3a35",
   "metadata": {},
   "source": [
    "> ## **비평(Commentary)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb066eec",
   "metadata": {},
   "source": [
    "<b></b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0473fb9b",
   "metadata": {},
   "source": [
    "> ## **참고 문헌(Reference)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df1e1bc",
   "metadata": {},
   "source": [
    "#### **(1) 관련 후속 논문** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ **[~]**\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ **[~]**\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ **[~]**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc4b681",
   "metadata": {},
   "source": [
    "#### **(2) 기타 참고 자료** :\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ **[~]**\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ **[~]**\n",
    "##### $ \\hspace{0.15cm} \\cdot{} $ **[~]**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
